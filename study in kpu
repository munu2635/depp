
cross - Entropy Error 

또다른 오차를 계싼하는 수식 방법 

p = 정답  
q = 예측값

=> -p(log(q)) # 아닌값을 제거하고 필요한 값만 사용 
q = 100퍼선트면 0
q의 정확도가 높아질수록 0에 수렴한다. 
q = 작아 질수록 값이 커짐 


08-22

mnist 학습모델 구현 (deep neural network)
 5 fully-connected layers (more then 1 hidden layer = deep layer )

1) sigmoid = hidden layers (1-4) // softmax = output layer (5)
EX202

slow start 현상 => sigmoid function은 vanishing gradient 문제를 발생 
시그모이드 함수를 사용하면 역전법을 진행시 신경망이 깊을 수록 인풋 레이어 에 가면 기울기가 0에 가까워지는 현상 => 시그모이드 대신 RELU, PRELU를 사용해서 하면 이런 현상을 줄일 수있다.

2) Relu = hidden layers (1-4) // softmax = output layer (5)
EX205
-- > slow start 문제 개선됨 
but noist accuracy curve 발생 
그래프를 확대해서 보면 accuracy 와 cross entropy loss 값이 변동 폭이 매우심함고 불안 
---> learning rate가 크기 떼문에 줄여서 해결 

3) Relu = hidden layers (1-4) // softmax = output layer (5) + Learning rate decay
EX207

>> learning_rate = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1/math.e)
이러한 모양으로 스탭 = 즉 수행 횟수에 따라서 값을 감소 하도록 설정하여 수행한다. 

학습률 그래프보다 손실율 그래프를 보고 계산하는게 훨씬 정확성을 잘 파악할수 있다. 

08-23
3) 이어서 
-> 시사점 일정 인식률(예 90%)에 도달하면 accuracy와 loss가 안정적으로 움직일 수 있도록 
learnubg rate를 줄여 나가는 것이 바람직

여기서 noisy 문제는 해결되었으나 overfitting 이슈 발생 
테스트 데이터에서 학습손실률을 따라가지 못함 ( 감소x 상승 o ) = overfitting 

4) overfitting 방지 방법
1 Regularization - Let's not have too big numbers in the weight 
2 Dropout 
- randomly 'drops' units from a layer on each training step creating 'sub-architecture' within the model
 사용 하지 않는 일부 뉴런을 삭제 
-can be viewed as a type of sampling of a smaller network with a larger network
------------------------------------------------------------------
첫 번째 : training data를 많이 모으는 것이다. 데이터가 많으면 training set, validation set, test set으로 나누어서 진행할 수 있고, 영역별 데이터의 크기도 커지기 때문에 overfitting 확률이 낮아진다. 달리 생각하면 overfitting을 판단하기가 쉬워지기 때문에 정확한 모델을 만들 수 있다고 볼 수도 있다.

두 번째 : feature의 갯수를 줄이는 것이다. 이들 문제는 multinomial classification에서도 다뤘었는데, 서로 비중이 다른 feature가 섞여서 weight에 대해 경합을 하면 오히려 좋지 않은 결과가 나왔었다. 그래서, feature 갯수를 줄이는 것이 중요한데, 이 부분은 deep learning에서는 중요하지 않다고 말씀하셨다. deep learning은 sigmoid 대신 LeRU 함수 계열을 사용하는 것도 있고 바로 뒤에 나오는 dropout을 통해 feature를 스스로 줄일 수 있는 방법도 있기 때문에.

세 번째 : regularization. 앞에서는 weight이 너무 큰 값을 갖지 못하도록 제한하기 위한 방법으로 설명했다. 이 방법을 사용하면 weight이 커지지 않기 때문에 선이 구부러지는 형태를 피할 수 있다.
-------------------------------------------------------------------------------------
4-1) drop out - like 시공이 많으면 배가 산으로 간다 
EX225
- 일부 뉴런을 제거하고 줄어든 신경망르을 통해 학습을 수행 
- 매번 미니배치 학습을 할때 마다  새로 랜덤으로 적용 
but testing 단계에서는 사용 x

pkeep ==> 삭제 퍼센트 

----------------------------------------
pkeep = tf.placeholder(tf.float32)
Yf = tf.nn.relu(tf.matmul(X, W) + B)
Y = tf.nn.dropout(Yf, pkeep)
-----------------------------------------

5) 1-4 번 까지 개선 시켰는데 아무리 노력해도 98.2% 이상의 인식률을 얻기 어려움 
-->>>>> 구조적 패러다임을 바꿔야 함 
지금까지 설명한 신경망 구조는 FCN (fully connect network)
이는 데이터 형상을 무시하는 문제점이 있다
 이미지를 학습 시킬때 이미지를 평평한 1차원데이터로 변환 시켜서하니까

 이는 cnn (Convolutional Nerual Network)로 해결 2-3 차원 이미지를 입력 받아 데이터 형상
 정보를 활용함으로써 fcn보다 정확하게 인식함 


----------------------------- cnn -----------------------------------------------------

1)cnn - Convolutional Neural Network 
주로 이미지 인식 분야에서 사용되는 딥러닝 네트위크 
이미지를 인식하는 알고리즘의 종류는 다양하게 있지만 모두 cnn에 기반을 두고 있다.  
single object    > Classification / + Localization
multiple object  > Object Detection / Instance Segmentation 

cnn 구조 
- cnn = convolution layer + pooling layer + fully connected layer 
1-1) Convolution 연산 
convolution == 의미적으로 (개념적으로) 사진에서 필터를 사용해 특징을 잡는 것이라고 생각 하면된다.
(계산적으로 convolve) 이동하면서 곱하고 적분한다 // 이동, 곱, 적분
컨볼루션 연산은 필터를 일정간격으로 이동해 가면서 입력데이터와 대응 원소간 곱셈을 실행한 후 총합을 구함 => 연산결과는 feature map 이라고 함 


1-2) pooling layer (= sub sampling )
필수가 아님 컨볼루션과 함게 사용 
학습 연산량을 줄이기 위해 feature 크기를 줄이는 과정을 추가함 
즉, 컨볼루션 계층에서 추출된 feature map 을 가로 세로 방향으로 축소하는 과정을 이야기함 
( pooling 연산은 가중치 합 연산이 아님 ) 
장점과 특징 
feature map 크기가 줄어들기 때문에 연산량을 줄일수 있다.  
입력데이터가 조금 변하더라도 풀링의 결과는 잘 변하지 않음  +  데이터 오버피팅 방지 효과가 있음 
+ 두가지가 많이 사용 Average pooling Max pooling -- max pooling을 많이사용 
무조건 좋은 건 아님 원래 갖고있는 피쳐맵의 정보를 잃어버린다고 볼 수 있는데 
정밀한 이미지를 적용할때는 사용할 수 없다.

1-3) fully connected layer
마지막으로 계산된 값이 어떤 사물인지 분석할때 사용한다. ==> 일반적으로 softmax 함수를 활성화 함수로 사용 
즉 추출된 특징들을 보고 클래스를 판단함 즉 최초 입력 이미지가 어떤 클래스에 속하는 지를 분류 

good training : learned fiters exhibit structure and are uncorrelated 
1-4) padding 
 컨볼루션 연산을 하기전에 입력데이터 주변을 특정값으로 채우는 것을 패딩이라함 
 출력크기를 조절할 대 사용, 즉 입력 데이터의 크기를 고정한 채로 다음 계층에 전달하기 위해 사용되는 기법

 1-5) stride 
 필터가 이동하는 간격을 스트라이드라고함 
 스트라이드를 2로 하면 필터가 2칸씩 이동하면서 컨볼루션 연산을 함 
 값을 크게 할 수록 출력 크기는 더 작아짐 

-- 출력 크기 계산 공식
입력크기 H, W // 필터크기 FH, FW // 출력크기 OH, OW // 패딩 P // 스트라이드 S 
OH = (H + 2P - FH)/S + 1  || OW = (H + 2P - FW)/S + 1  -- 주의 OH, OW는 원소의 개수를 나타냄 따라서 정수값이여함 

2) cnn 구현 

2-1) cnn 구조 설계 - 내가 설계 한다
각 layer의 뉴련 개수는 이전 layer보다 대략 절반 크기로 줄도록 설계하는 것이 좋음 
ex 28x28x4 => 3000 -> 14x14x8 => 1500 -> 7x7x12 => 500 -> 200 --softmax--> 10 
	 4개의 특징을 구함  8개의 특징을 구함   12개의 특징을 구함 


conv2d 함수의 샬먕을 찾아본당 
필터크기와 채널 갯수를 조정하여 설계 

2-#) 가중치 vs 하이퍼파라미터 
- 가중치 weight & 편향 bias == 딥러닝 알고리즘에 의해 자동으로 최적화
 딥러닝 알고리즘 (SGD)에 의해 자동적으로 최적값이 결정됨 
 딥러닝 알고리즘이 학습데이터로 부터 최적 가중치값을 자동으로 계산함

- 하이퍼 파리미터 hyper Parameter == 딥러닝 개발자가 최적값을 수동으로 설정함 
 하이퍼 파라미터는 뉴런수, 배치크기, 매개변수 갱신시의 학습률, epoch 수 등이 있음
 cnn 에서는 필터크기, 채널개수도 하이퍼 파라미터 
 딥러닝 개발자가 최적값을 수동으로 선택해야함 

08-24 last-day

3) Bigger CNN 구현 - 실습한 내용에 대해서 설명
cnn 구조변경 필터크기와 채널갯수를 조정하여 cnn구조를 전체적으로 변경  layer 1 -> 28x28x6
dropout 구현 

4) Tensorboard 

5) 주요 CNN 모델 

6) 딥러닝 기술과 응용 분야
	CNN based Detection 
	1 - Image Classification (CNN)
	2 - object Detection (Faster RCNN, SSD, YOLO)
	3 - Instance Segmentation (Mask RCNN)
	4 - 자연어 처리 (RNN, LSTM)
	5 - 이미지 생성 (GAN)
	6 - Reinforcement Learning (DQN, policy Gradient)

+-1) Epoch vs Iteration 

1 Epoch 		모든 데이터 셋을 한 번 학습 
1 iteration 	1회 학습 
minibatch 		데이터 셋을 batch size크기로 쪼개서 학습 
--------------------------------------------
ex) 총 데이터가 100개, batch size가 10 이면 
1 iteration = 10개 데이터에 대해서 학습 
1 Epoch 	= 100/batch size = 10 iteration
--------------------------------------------

+-2) 데이터 셋 분할 방법
학습 데이터와 시험 데이터로 분할하는 방법 
  - 학습데이터 : 매개 변수 학습에 이용 
  - 검증데이터 : 하이퍼파리미터 최적값을 찾는데 이용 
  - 시험데이터 : 학습모댈이 학습데이터에 오버피팅 여부 혹은 성능이 어느정도인지 평가하는 용도 


+-3)Activation Function 선택 
- 분류(classificaion) vs 회귀(regression)  ++++ 군집화(clustering), 연관성분석(association)
 	분류 = 데이터가 어느 클래스에 속하는지를 판단하는 기법
 	회귀 = 데이터들로 부터 일정한 패턴을 찾은 후 예측하는 기법 

- 출력층의 활성화 함수 선택
	2 클래스 분류 문제 	: Sigmoid 사용 
	다중클래스 분류 문제 	: Softmax 사용 
	회귀 문제 			: Output 그대로 사용 

참고교재 
deep-Learning from scratch
Fensorflow and deep learning :
https://github.com/GoogleCloudPlatform/tensorflow-without-a-phd/tree/master/tensorflow-mnist-tutorial